{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import os, os.path\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "import numpy as np\n",
    "import copy\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring stopwords from file \n",
    "docs = []\n",
    "PositionIndexArray = []\n",
    "InvertedIndexArray = []\n",
    "docnum = 0\n",
    "with open(\"stopwords.txt\" , 'r', encoding=\"utf-8\") as f:\n",
    "    stops = []\n",
    "    for line in f:\n",
    "        line = line[:-1]\n",
    "        stops.append(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get path of file for reading \n",
    "paths = []\n",
    "title = \"dataset/cars\"\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/2007'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/2008'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/2009'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add every txet file as a doc to docs\n",
    "def addDoc(text , name,docno):\n",
    "    ch = True\n",
    "    for doc in docs:\n",
    "        if doc.id == name:\n",
    "            ch = False\n",
    "            break\n",
    "    if ch:\n",
    "        try:    \n",
    "            docs.append(prepareData(text,name,docno))\n",
    "        except:\n",
    "            a = 5\n",
    "    else:\n",
    "        print('this doc has been added!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process such as tokenize , convert numbers , convert to lower ,stemming , remove stopwords , remove single character and bad characters\n",
    "def prepareData(text,name , docno):\n",
    "    res = []\n",
    "    lines = text\n",
    "    lines = convert_numbers(lines)\n",
    "    lines = convert_lower_case(lines)\n",
    "    stemmer = PorterStemmer()\n",
    "    lines = str(lines)\n",
    "    lines = nltk.word_tokenize(lines)   \n",
    "    line2 = []\n",
    "    for w in lines:\n",
    "        if w not in stops:\n",
    "            w = stemmer.stem(w)\n",
    "            line2.append(w)\n",
    "    res.append(line2)\n",
    "    res = remove_single_characters(res)\n",
    "    res = remove_punctuation(res)\n",
    "    doc = Doc(text,name,res,docno)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for convert number\n",
    "def convert_numbers(data):\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\n",
    "    data = np.char.replace(data, \"1\", \" one \")\n",
    "    data = np.char.replace(data, \"2\", \" two \")\n",
    "    data = np.char.replace(data, \"3\", \" three \")\n",
    "    data = np.char.replace(data, \"4\", \" four \")\n",
    "    data = np.char.replace(data, \"5\", \" five \")\n",
    "    data = np.char.replace(data, \"6\", \" six \")\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for convert data to lower case\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for remove single characters\n",
    "def remove_single_characters(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for remove ' from data\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for remover bad char\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"![\\\"#$%&()*+-./:;<=>?@\\^_`{|}~] ' ''\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "    data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, \"   \", \" \")\n",
    "    data = np.char.replace(data,\"    \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    data = np.char.replace(data, '...', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a class for each text file\n",
    "class Doc:\n",
    "    def __init__(self,lines,id,main,num):\n",
    "        self.lines = lines\n",
    "        self.id = id\n",
    "        self.main = main\n",
    "        self.num = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the path for reading files , preprocess and create class for each file \n",
    "#user path[57:] for name because path contain all directory and we nedd just name of file\n",
    "#docnum is number of doc we seted for it\n",
    "def readlAlldata():\n",
    "    docno = 0\n",
    "    for path in paths:\n",
    "        file = open(path, 'r', encoding='cp1250')\n",
    "        text = file.read().strip()\n",
    "        name = path[56:]\n",
    "        docno += 1\n",
    "        file.close()\n",
    "        addDoc(text, name , docno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "readlAlldata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all words from docs that we create stop this specific word and remove duplicate words\n",
    "allwords = []\n",
    "common_words = [\"docno\",\"doc\",\"date\",\"author\", \"text\", \"favorit\",\"zero\",\"one\",\"two\" ,\"three\",\"four\",\"five\",\"six\", \"seven\",\"eight\",\"nine\"] \n",
    "for doc in docs:\n",
    "    temp = doc.main\n",
    "    temp = str(temp)\n",
    "    temp = remove_punctuation(temp)\n",
    "    temp = str(temp)\n",
    "    temp = remove_single_characters(temp)\n",
    "    temp = str(temp)\n",
    "    temp = temp.split()\n",
    "    for word in temp:\n",
    "        if word not in common_words:\n",
    "            if word not in allwords:\n",
    "                allwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a inverted index class for each word\n",
    "class InInd:\n",
    "    def __init__(self,word,abundance,posting):\n",
    "        self.word = word\n",
    "        self.abundance = abundance\n",
    "        self.posting = posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create posting list for inverted index \n",
    "def addWordToWords1(word1):\n",
    "    check = True\n",
    "    for wo in InvertedIndexArray:\n",
    "        if wo.word == word1:\n",
    "            check = False\n",
    "            print(\"word is checked!\")\n",
    "            break\n",
    "    if check:\n",
    "        posting=[]\n",
    "        index = []\n",
    "        docfreq = 0\n",
    "        for doc in docs:\n",
    "            checkInDoc = False\n",
    "            temp = doc.main\n",
    "            temp = str(temp)\n",
    "            if word1 in temp:\n",
    "                checkInDoc = True\n",
    "                docfreq +=1\n",
    "                if checkInDoc :\n",
    "                    id = doc.num\n",
    "                    posting.append(id)\n",
    "        index = InInd(word1,docfreq,posting)\n",
    "        InvertedIndexArray.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add all word to posting list\n",
    "for i in allwords:\n",
    "    i = remove_punctuation(i)\n",
    "    i = str(i)\n",
    "    addWordToWords1(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write invertedt index in text file\n",
    "with open(\"Inverted_index.txt\" , 'w') as f:\n",
    "    for i in InvertedIndexArray:\n",
    "        f.write(str(i.word))\n",
    "        f.write(\" : \")\n",
    "        f.write(str(i.abundance))\n",
    "        f.write(\" ------> \")\n",
    "        f.write(str(i.posting))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create post of all words DF and TF \n",
    "def addWordToWords(word1):\n",
    "    check = True\n",
    "    for wo in PositionIndexArray:\n",
    "        if wo.word == word1:\n",
    "            check = False\n",
    "            print(\"word is checked!\")\n",
    "            break\n",
    "    term = 0;    \n",
    "    if check:\n",
    "        docposes=[]\n",
    "        for doc in docs:\n",
    "            countInDoc = 0\n",
    "            checkInDoc = False\n",
    "            postions = []\n",
    "            temp = str(doc.main)\n",
    "            postions = [i+1 for i,w in enumerate(temp.split()) if w == word1]\n",
    "            if postions != \"[]\":\n",
    "                if len(postions) != 0:\n",
    "                    checkInDoc = True\n",
    "                    countInDoc = len(postions)\n",
    "                    term = term + countInDoc\n",
    "                    if checkInDoc :\n",
    "                        id = doc.id\n",
    "                        docpos = [id,countInDoc,postions]\n",
    "                        docposes.append(docpos)\n",
    "        posInd = PosInd(word1,term,docposes)\n",
    "        PositionIndexArray.append(posInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a positinal index class for each word\n",
    "class PosInd:\n",
    "    def __init__(self,word,abundance,docPositions):\n",
    "        self.word = word\n",
    "        self.abundance = abundance\n",
    "        self.docPositions = docPositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass all the words for create inverted index\n",
    "for i in allwords:\n",
    "    i = remove_punctuation(i)\n",
    "    addWordToWords(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because of saving doc name in positinal index text and use that for finding information this method extract doc number uses doc name\n",
    "def finddocnum(id):\n",
    "    for doc in docs:\n",
    "        if doc.id == id:\n",
    "            return doc.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create posInd class and PositionIndexArray from saved posting lis\n",
    "#create this list took 31 hours to create after one time cration i saved it and use that for saving time\n",
    "with open(\"posting_list.txt\" , 'r') as f:\n",
    "    document = f.read()\n",
    "word = \"\"\n",
    "termfreq = \"\"\n",
    "postintion = []\n",
    "for line in document.split(\"\\n\"):\n",
    "    if \"word : \" in line:\n",
    "        word = line[7:]\n",
    "    elif \"docs : \" in line:\n",
    "        termfreq = line[39:]\n",
    "    elif \"positions : \" in line:\n",
    "        postintion = line[90:]\n",
    "    if word != \"\" and postintion != [] and termfreq != \"\":\n",
    "        posInd = PosInd(word,termfreq,postintion)\n",
    "        PositionIndexArray.append(posInd)\n",
    "        word = \"\"\n",
    "        termfreq = \"\"\n",
    "        postintion = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text with tokenid , docid and term frequency in that doc\n",
    "termfreqindoc = []\n",
    "tokenid = 0\n",
    "file = open(\"termfreq.txt\" ,'w')\n",
    "for i in PositionIndexArray:\n",
    "    temp = str(i.docPositions)\n",
    "    tokenid += 1\n",
    "    for word in temp.split(\", [\"):\n",
    "        if \"'\" in word:\n",
    "            termfreq = []\n",
    "            for w in word.split(\",\"):\n",
    "                w = w.replace(\"[\",\"\")\n",
    "                w = w.replace(\"'\",\"\")\n",
    "                termfreq.append(w)\n",
    "            docnumber = finddocnum(termfreq[0])\n",
    "            file.write(str(tokenid))\n",
    "            file.write(\" , \")\n",
    "            file.write(str(docnumber))\n",
    "            file.write(\" , \")\n",
    "            file.write(str(termfreq[1]))\n",
    "            file.write(\"\\n\")\n",
    "    file.write(\".......................................................\")\n",
    "    file.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 598], [6, 598], [44, 598], [56, 598], [68, 598], [141, 598], [175, 598], [280, 598], [295, 598], [394, 598], [410, 598], [476, 598], [488, 598], [493, 598], [511, 598], [719, 598], [724, 598], [738, 598], [776, 598], [784, 598], 'comfort', 'drive', 'ac', 'car', 'great', 'on', 'interior', 'love', 'int', 'ad', 'ex', 'ca', 'ov', 'up', 'it', 've', 'll', 'and', 'to', 'no']\n"
     ]
    }
   ],
   "source": [
    "#extract document frequency and document name for find docid\n",
    "#extract 20 word with most doc frequncy and thieri id and number of doc\n",
    "docfrequ = []\n",
    "for i in InvertedIndexArray:\n",
    "    docfrequ.append(i.abundance)\n",
    "temp = []\n",
    "temp = docfrequ.copy()\n",
    "temp.sort()\n",
    "new_temp = temp[-21:-1]\n",
    "wordid = []\n",
    "last20=0\n",
    "docfrequ_wordid = []\n",
    "for j in range(0,len(new_temp)):\n",
    "    for i in range(0,len(docfrequ)):\n",
    "        if new_temp[j] == docfrequ[i]:\n",
    "            if last20 <20:\n",
    "                m = [i,new_temp[j]]\n",
    "                wordid.append(i)\n",
    "                docfrequ_wordid.append(m)\n",
    "                last20 += 1\n",
    "checkid = 0\n",
    "for i in InvertedIndexArray:\n",
    "    if checkid in wordid:\n",
    "        docfrequ_wordid.append(i.word) \n",
    "    checkid+=1\n",
    "print(docfrequ_wordid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text file with tokenid and tokenstring and document frequency\n",
    "with open(\"docfrequ.txt\" ,'w') as f:\n",
    "    tokenid = 0\n",
    "    for i in InvertedIndexArray:\n",
    "        f.write(\"tokenID : \")\n",
    "        f.write(str(tokenid+1))\n",
    "        f.write(\" -----> tokenString : \")\n",
    "        f.write(i.word)\n",
    "        f.write(\" -----> ‫‪DocumentFrequency‬‬ : \")\n",
    "        f.write(str(i.ababundance))\n",
    "        f.write(\"\\n\")\n",
    "        tokenid+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for finding 20 of largest part in list\n",
    "def Nnumberele(list1, N): \n",
    "    new_list = [] \n",
    "    for i in range(0, N):  \n",
    "        max1 = 0\n",
    "        for j in range(len(list1)):      \n",
    "            if list1[j] > max1: \n",
    "                max1 = list1[j]; \n",
    "        list1.remove(max1); \n",
    "        new_list.append(max1) \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract 20 word with most term frequency and number of repeat\n",
    "x = 0\n",
    "termfr20 = []\n",
    "w = []\n",
    "for i in PositionIndexArray:\n",
    "    termfr20.append(i.abundance)\n",
    "for i in range(len(termfr20)):\n",
    "    termfr20[i] = int(termfr20[i])\n",
    "temp = Nnumberele(termfr20,20)\n",
    "for m in range(0,20):\n",
    "    for i in PositionIndexArray:\n",
    "        if int(i.abundance) == temp[m]:\n",
    "            twen = [ i.word ,temp[m]]\n",
    "            w.append(twen)  \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all brands of car from our files\n",
    "def car_brands():\n",
    "    brands = []\n",
    "    for path in paths:\n",
    "        if path[61:] not in brands:\n",
    "            brands.append(path[61:])\n",
    "    print(brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of cars per years  and all cars\n",
    "def numberOfCarsPerYear():\n",
    "    i2007 =0\n",
    "    i2008=0\n",
    "    i2009=0\n",
    "    for path in paths:\n",
    "        docid = str(path[56:])\n",
    "        if \"2007\" in docid:\n",
    "            i2007 = i2007+1\n",
    "        elif docid.__contains__('2008'):\n",
    "            i2008 = i2008 + 1\n",
    "        elif docid.__contains__('2009'):\n",
    "            i2009 = i2009 + 1\n",
    "    iallcar = i2007 + i2008 + i2009   \n",
    "    stat = 'number of cars in 2007: '+str(i2007)+\"\\nnumber of cars in 2008: \"+str(i2008)+\"\\nnumber of cars in 2009: \"+str(i2009)+\"\\nnumber of all cars : \"+str(iallcar)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of comments per year and all year\n",
    "def comment_per_year():\n",
    "    c2007 = 0\n",
    "    c2008 = 0\n",
    "    c2009 = 0\n",
    "    for doc in docs:\n",
    "        docid = str(doc.id)\n",
    "        docmain = str(doc.main)\n",
    "        if \"2007\" in docid:   \n",
    "            c2007 = len(re.findall('text', docmain))/2 + c2007\n",
    "        if \"2008\" in docid:\n",
    "            c2008 = len(re.findall('text', docmain))/2 + c2008\n",
    "        if \"2009\" in docid:\n",
    "            c2009 = len(re.findall('text', docmain))/2 + c2009\n",
    "    c2007 = int(c2007)\n",
    "    c2008 = int(c2008)\n",
    "    c2009 = int(c2009)\n",
    "    c_all = c2007 + c2008 + c2009\n",
    "    stat = 'number of comment in 2007: '+str(c2007)+\"\\nnumber of comment in 2008: \"+str(c2008)+\"\\nnumber of comment in 2009: \"+str(c2009)+\"\\nnumber of comment in all years: \"+str(c_all)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number od comments per year\n",
    "def comment_per_car():\n",
    "    nc_car = []\n",
    "    for doc in docs:\n",
    "        c2007 = 0\n",
    "        docid = str(doc.id)\n",
    "        docmain = str(doc.main)\n",
    "        for word in brands:\n",
    "            word = str(word)\n",
    "            word = word.replace('[','')\n",
    "            word = word.replace(']','')\n",
    "            word = word.replace(\"'\",\"\")\n",
    "            word = word.replace(\",\",\"\")\n",
    "            if word == docid[5:]:\n",
    "                if \"2007\" in docid:\n",
    "                    c2007 = len(re.findall('text', docmain))/2\n",
    "                    c2007 = int(c2007)\n",
    "                    comcar = [word,\"2007\",c2007]\n",
    "                    nc_car.append(comcar)\n",
    "                elif \"2008\" in docid:\n",
    "                    c2007 = len(re.findall('text', docmain))/2\n",
    "                    c2007 = int(c2007)\n",
    "                    comcar = [word,\"2008\",c2007]\n",
    "                    nc_car.append(comcar)\n",
    "                elif \"2009\" in docid:\n",
    "                    c2007 = len(re.findall('text', docmain))/2\n",
    "                    c2007 = int(c2007)\n",
    "                    comcar = [word,\"2009\",c2007]\n",
    "                    nc_car.append(comcar)\n",
    "    print(nc_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of comments for each car in all years\n",
    "def comment_allcar():\n",
    "    all_cars = []\n",
    "    for word in brands:\n",
    "        word = str(word)\n",
    "        c2007 = 0\n",
    "        word = word.replace('[','')\n",
    "        word = word.replace(']','')\n",
    "        word = word.replace(\"'\",\"\")\n",
    "        word = word.replace(\",\",\"\")\n",
    "        for doc in docs:\n",
    "            docid = str(doc.id)\n",
    "            docmain = str(doc.main)\n",
    "            if word == docid[5:]:\n",
    "                c2007 = len(re.findall('text', docmain))/2 +c2007\n",
    "                c2007 = int(c2007)\n",
    "        nomcar = [word,c2007]\n",
    "        all_cars.append(nomcar)\n",
    "    print(all_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_brands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfCarsPerYear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_per_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_per_car()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_allcar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
